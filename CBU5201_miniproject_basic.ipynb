{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d4a8ad",
   "metadata": {},
   "source": [
    "# CBU5201 mini-project submission\n",
    "\n",
    "The mini-project has two separate components:\n",
    "\n",
    "\n",
    "1.   **Basic component** [6 marks]: Using the genki4k dataset, build a machine learning pipeline that takes as an input an image and predicts 1) whether the person in the image is similing or not 2) estimate the 3D head pose labels in the image.\n",
    "2.   **Advanced component** [10 marks]: Formulate your own machine learning problem and build a machine learning solution using the genki4k dataset (https://inc.ucsd.edu/mplab/398/). \n",
    "\n",
    "Your submission will consist of two Jupyter notebooks, one for the basic component and another one for advanced component. Please **name each notebook**:\n",
    "\n",
    "* CBU5201_miniproject_basic.ipynb\n",
    "* CBU5201_miniproject_advanced.ipynb\n",
    "\n",
    "then **zip and submit them toghether**.\n",
    "\n",
    "Each uploaded notebook should include: \n",
    "\n",
    "*   **Text cells**, describing concisely each step and results.\n",
    "*   **Code cells**, implementing each step.\n",
    "*   **Output cells**, i.e. the output from each code cell.\n",
    "\n",
    "and **should have the structure** indicated below. Notebooks might not be run, please make sure that the output cells are saved.\n",
    "\n",
    "How will we evaluate your submission?\n",
    "\n",
    "*   Conciseness in your writing (10%).\n",
    "*   Correctness in your methodology (30%).\n",
    "*   Correctness in your analysis and conclusions (30%).\n",
    "*   Completeness (10%).\n",
    "*   Originality (10%).\n",
    "*   Efforts to try something new (10%).\n",
    "\n",
    "Suggestion: Why don't you use **GitHub** to manage your project? GitHub can be used as a presentation card that showcases what you have done and gives evidence of your data science skills, knowledge and experience. \n",
    "\n",
    "Each notebook should be structured into the following 9 sections:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a94be-01e1-4c27-b7a3-accc13929b2f",
   "metadata": {},
   "source": [
    "# 1 Author\n",
    "\n",
    "**Student Name**: yida Wang \n",
    "**Student ID**:  210978645(QMUL) 2021212787(BUPT)\n",
    "\n",
    "**Access this code from github** https://github.com/akawangyida/BUPT-mini-project.git\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871eff0",
   "metadata": {},
   "source": [
    "# 2 Problem formulation\n",
    "\n",
    "Describe the machine learning problem that you want to solve and explain what's interesting about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273d563",
   "metadata": {},
   "source": [
    "Developing a Facial Expression and Pose Analysis System: In this initiative, the goal is to build a sophisticated machine learning model leveraging the genki4k dataset. This model is tasked with dual functions: identifying whether the person in an image is smiling, and gauging the 3D orientation of the person's head. The key challenge here is to precisely capture and interpret nuanced facial expressions and the directional positioning of the head. This endeavor holds considerable potential in enhancing interactive technologies and contributing to research in the field of human-machine interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7829fc",
   "metadata": {},
   "source": [
    "# 3 Machine Learning pipeline\n",
    "\n",
    "Development of a Facial Analysis System: The challenge is to create a computational model using the genki4k dataset. This model should analyze an input image to determine two key aspects: firstly, whether the individual in the image is smiling, and secondly, to compute the orientation of the head in three-dimensional space. This problem is fascinating as it combines emotion recognition with spatial orientation analysis, showcasing the potential of AI in understanding human expressions and poses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca88ba9",
   "metadata": {},
   "source": [
    "# 4 Transformation stage\n",
    "\n",
    "Describe any transformations, such as feature extraction. Identify input and output. Explain why you have chosen this transformation stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef20cff",
   "metadata": {},
   "source": [
    "Transformation Stage: Feature Extraction\n",
    "Input:\n",
    "Preprocessed Images: The input to this stage is the array of preprocessed images. These images have already been resized to 64x64 pixels and normalized (pixel values scaled to the range [0, 1]).\n",
    "Feature Extraction Process:\n",
    "Convolutional Layers (Conv2D):\n",
    "\n",
    "The CNN employs multiple convolutional layers.\n",
    "Each layer uses a set of learnable filters to capture various aspects of the image, such as edges, textures, and other patterns.\n",
    "The activation function 'relu' (Rectified Linear Unit) introduces non-linearity, allowing the model to learn more complex features.\n",
    "Pooling Layers (MaxPooling2D):\n",
    "\n",
    "After each convolutional layer, a pooling layer reduces the spatial dimensions (height and width) of the output, condensing the feature maps.\n",
    "This reduction helps in decreasing the computational load and the number of parameters, making the network less prone to overfitting.\n",
    "Flattening:\n",
    "\n",
    "The Flatten layer is used to convert the 2D feature maps into a 1D vector. This transformation is necessary to feed the data into the dense layers for classification.\n",
    "Dense Layers and Dropout:\n",
    "\n",
    "Dense layers further process the features, culminating in a classification output.\n",
    "Dropout is applied to prevent overfitting by randomly setting a fraction of the input units to zero during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22a73b",
   "metadata": {},
   "source": [
    "# 5 Modelling\n",
    "\n",
    "Describe the ML model(s) that you will build. Explain why you have chosen them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f456ac",
   "metadata": {},
   "source": [
    "CNN Model Description:\n",
    "Convolutional Layers (Conv2D):\n",
    "These layers are the core building blocks of a CNN. They are effective in extracting features from images by applying various filters that capture aspects like edges, textures, and patterns.\n",
    "Pooling Layers (MaxPooling2D):\n",
    "These layers follow the convolutional layers and are used to reduce the spatial size of the representation. This reduction helps in decreasing the computational power required, while also helping to extract dominant features by reducing noise.\n",
    "Flattening:\n",
    "The Flatten layer is used to convert the 2D feature maps into a 1D vector, which is necessary for the final classification step.\n",
    "Dense Layers and Dropout:\n",
    "After flattening, one or more dense layers are used for classification. The dropout layer is included to prevent overfitting by randomly dropping a fraction of the neurons.\n",
    "Output Layer:\n",
    "The final Dense layer uses a 'softmax' activation function for multi-class classification or 'sigmoid' for binary classification, providing the probability of the image belonging to each class.\n",
    "Reasons for Choosing a CNN:\n",
    "Specialization in Image Processing:\n",
    "CNNs are specifically designed for processing data with a grid-like topology, such as images. They are highly efficient in handling image data due to their ability to capture spatial hierarchies.\n",
    "Feature Extraction Capability:\n",
    "CNNs automatically and adaptively learn spatial hierarchies of features. This is crucial for image classification tasks where manual feature extraction is complex and inefficient.\n",
    "Robustness and Accuracy:\n",
    "They are known for their robustness to variations in the input and have been proven to achieve high accuracy in various image classification tasks.\n",
    "Scalability:\n",
    "CNNs can be scaled easily in terms of depth and complexity, allowing for fine-tuning and optimization based on the specific requirements of the dataset and task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ec26b",
   "metadata": {},
   "source": [
    "# 6 Methodology\n",
    "\n",
    "Describe how you will train and validate your models, how model performance is assesssed (i.e. accuracy, confusion matrix, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbca184",
   "metadata": {},
   "source": [
    "For the first task, the model's performance will be evaluated using accuracy and a classification report (class_report). Accuracy will provide a quick overview of the overall effectiveness of the model, while the classification report will offer detailed insights into precision, recall, and F1-score for each class, allowing for a more nuanced understanding of the model's performance.\n",
    "\n",
    "For the second task, Mean Squared Error (MSE) will be used as the key performance metric. MSE will offer a clear indication of how close the model's predictions are to the actual values, with lower values indicating better performance. This metric is particularly useful for regression tasks or models predicting continuous outco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f52ae",
   "metadata": {},
   "source": [
    "# 7 Dataset\n",
    "\n",
    "Describe the dataset that you will use to create your models and validate them. If you need to preprocess it, do it here. Include visualisations too. You can visualise raw data samples or extracted features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325b805",
   "metadata": {},
   "source": [
    "The dataset to be used for the models comprises images along with their corresponding labels. These images are likely varied in content and need to be preprocessed for effective model training and validation.\n",
    "\n",
    "Preprocessing Steps:\n",
    "Loading and Resizing: Images are loaded from a directory, converted to a consistent size (like 64x64 pixels) for uniformity in input.\n",
    "Normalization: Pixel values are normalized to a range of 0 to 1 to facilitate efficient training.\n",
    "Reshaping: Images are reshaped to match the input requirements of the CNN (adding a channel dimension for RGB images).\n",
    "Visualization:\n",
    "Sample Images: Display a few images from the dataset to understand the diversity and characteristics of the data.\n",
    "Label Distribution: Visualize the distribution of different labels (classes) in the dataset to identify any imbalance.\n",
    "These visualizations help in understanding the dataset better, ensuring that the preprocessing steps are aligned with the needs of the models to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb288f",
   "metadata": {},
   "source": [
    "# 8 Results\n",
    "\n",
    "Carry out your experiments here, explain your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27fc129",
   "metadata": {},
   "source": [
    "Task 1: Smile Detection\n",
    "Test Accuracy: 79.37%\n",
    "Classification Report:\n",
    "Class0 (Not Smiling): Precision - 76%, Recall - 77%, F1-score - 77%.\n",
    "Class1 (Smiling): Precision - 82%, Recall - 81%, F1-score - 81%.\n",
    "Analysis:\n",
    "The model performs fairly well in distinguishing smiling from non-smiling faces, with a slightly better performance in correctly identifying smiling faces (Class1).\n",
    "The balance between precision and recall for both classes indicates a good model performance, with slightly better results for Class1.\n",
    "\n",
    "Task 2: 3D Head Pose Estimation\n",
    "Test Mean Squared Error (MSE): 0.01763\n",
    "R-squared: 0.33458\n",
    "Analysis:\n",
    "The MSE is relatively low, suggesting that the model's predictions are, on average, close to the actual 3D head pose values. However, the interpretation of MSE should be contextual to the range and scale of the head pose angles.\n",
    "The R-squared value, a measure of how well the observed outcomes are replicated by the model, is around 33.46%. While this indicates some level of predictive power, it also suggests there is significant room for improvement, as a higher R-squared value would be desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94826b",
   "metadata": {},
   "source": [
    "# 9 Conclusions\n",
    "\n",
    "Your conclusions, improvements, etc should go here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa941f3",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "Smile Detection:\n",
    "Performance: The model achieves a decent accuracy of 79.37% in smile detection.\n",
    "Strengths: It shows slightly better precision and recall for detecting smiles (Class1) than non-smiles (Class0), indicating effectiveness in recognizing smiling faces.\n",
    "Balanced Metrics: The close alignment of precision, recall, and F1-scores for both classes suggests a balanced performance across both categories.\n",
    "3D Head Pose Estimation:\n",
    "MSE and R-squared: The model's Mean Squared Error (MSE) is reasonably low at 0.01763, but the R-squared value is at 33.46%, indicating moderate predictive accuracy.\n",
    "Room for Improvement: The R-squared value suggests that the model is capturing some, but not all, of the variability in the 3D head pose data.\n",
    "Suggested Improvements:\n",
    "\n",
    "For Smile Detection:\n",
    "Data Augmentation: More aggressive augmentation strategies could help the model generalize better, especially for the non-smiling class.\n",
    "Model Complexity: Experimenting with deeper or more complex CNN architectures could enhance the model's ability to capture subtle features indicative of smiling or not smiling.\n",
    "\n",
    "For 3D Head Pose Estimation:\n",
    "Advanced Architectures: Implementing more advanced or deeper neural network architectures might capture the complexities of 3D head pose estimation more effectively.\n",
    "Feature Engineering: Investigating additional preprocessing or feature engineering techniques to enhance the representativeness of the input data.\n",
    "Hyperparameter Tuning: Systematic tuning of hyperparameters like learning rate, batch size, and the number of epochs could lead to improvements in model performance.\n",
    "General Improvements:\n",
    "Cross-Validation: Implementing k-fold cross-validation could provide a more robust evaluation of the model's performance.\n",
    "Regularization Techniques: Applying regularization methods like dropout or L1/L2 regularization could help prevent overfitting, particularly for the 3D head pose estimation model.\n",
    "Post-Model Analysis: A detailed analysis of misclassified instances or poorly estimated poses could provide insights into specific areas where the models are underperforming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc249373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c78f1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4000 images and 4000 labels.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def Llabels(labels_path):\n",
    "    \"\"\"Load labels from a text file.\"\"\"\n",
    "    labels = np.loadtxt(labels_path)\n",
    "    return labels\n",
    "\n",
    "def Limages(directory, image_size=(64, 64)):\n",
    "    \"\"\"Load images from a directory, resize and convert them to numpy arrays.\"\"\"\n",
    "    image_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.jpg')]\n",
    "    images = [Image.open(file).convert('RGB').resize(image_size) for file in image_files]\n",
    "    images = np.array([np.array(image) for image in images])\n",
    "    return images\n",
    "\n",
    "\n",
    "# Set the dataset directory and paths for files and labels\n",
    "dataset_dir = '.' \n",
    "files_dir = os.path.join(dataset_dir, 'files')\n",
    "labels_file = os.path.join(dataset_dir, 'labels.txt')\n",
    "\n",
    "# Load data\n",
    "labels = Llabels(labels_file)\n",
    "images = Limages(files_dir)\n",
    "images = images/255\n",
    "\n",
    "print(f\"Loaded {len(images)} images and {len(labels)} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05b98897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 6s 54ms/step - loss: 0.6904 - accuracy: 0.5469\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 0.6767 - accuracy: 0.5756\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 0.6639 - accuracy: 0.6028\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 0.6527 - accuracy: 0.6216\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 0.6459 - accuracy: 0.6313\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 0.6043 - accuracy: 0.6747\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 0.5486 - accuracy: 0.7147\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.4545 - accuracy: 0.7944\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 0.3923 - accuracy: 0.8278\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 0.3422 - accuracy: 0.8528\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.2791 - accuracy: 0.8794\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.2390 - accuracy: 0.9028\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.2021 - accuracy: 0.9194\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 0.1567 - accuracy: 0.9413\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 0.1243 - accuracy: 0.9569\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 0.0893 - accuracy: 0.9675\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 0.0801 - accuracy: 0.9694\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 0.0630 - accuracy: 0.9759\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 0.0508 - accuracy: 0.9812\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 0.0561 - accuracy: 0.9809\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.8023 - accuracy: 0.7937\n",
      "Test accuracy: 0.793749988079071\n",
      "25/25 [==============================] - 0s 11ms/step\n",
      "25/25 [==============================] - 0s 11ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Class0       0.76      0.77      0.77       354\n",
      "      Class1       0.82      0.81      0.81       446\n",
      "\n",
      "    accuracy                           0.79       800\n",
      "   macro avg       0.79      0.79      0.79       800\n",
      "weighted avg       0.79      0.79      0.79       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "images_train_, images_test, labels_train_, labels_test = train_test_split(images, labels[:,0], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a simple CNN model\n",
    "def create_custom_cnn():\n",
    "    custom_model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    return custom_model\n",
    "\n",
    "custom_cnn_model = create_custom_cnn()\n",
    "\n",
    "# Compile the model\n",
    "custom_cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "training_history = custom_cnn_model.fit(images_train_, labels_train_, epochs=20)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = custom_cnn_model.evaluate(images_test, labels_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predictions\n",
    "predictions_custom_cnn = custom_cnn_model.predict(images_test)\n",
    "\n",
    "\n",
    "# Predict class labels on the test set\n",
    "predicted_probabilities = custom_cnn_model.predict(images_test)\n",
    "predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "\n",
    "# Generate the classification report\n",
    "class_report = classification_report(labels_test, predicted_classes, target_names=['Class0', 'Class1'])\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38319771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def create_regression_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3)  # Output layer for 3D head pose regression (3 continuous values)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "regression_cnn_model = create_regression_cnn()\n",
    "\n",
    "# Compile the model for regression\n",
    "regression_cnn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18936dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "75/75 [==============================] - 5s 62ms/step - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0215 - val_mean_squared_error: 0.0215\n",
      "Epoch 2/20\n",
      "75/75 [==============================] - 5s 63ms/step - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0190 - val_mean_squared_error: 0.0190\n",
      "Epoch 3/20\n",
      "75/75 [==============================] - 5s 61ms/step - loss: 0.0156 - mean_squared_error: 0.0156 - val_loss: 0.0199 - val_mean_squared_error: 0.0199\n",
      "Epoch 4/20\n",
      "75/75 [==============================] - 5s 65ms/step - loss: 0.0145 - mean_squared_error: 0.0145 - val_loss: 0.0189 - val_mean_squared_error: 0.0189\n",
      "Epoch 5/20\n",
      "75/75 [==============================] - 4s 58ms/step - loss: 0.0132 - mean_squared_error: 0.0132 - val_loss: 0.0181 - val_mean_squared_error: 0.0181\n",
      "Epoch 6/20\n",
      "75/75 [==============================] - 4s 57ms/step - loss: 0.0117 - mean_squared_error: 0.0117 - val_loss: 0.0179 - val_mean_squared_error: 0.0179\n",
      "Epoch 7/20\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
      "Epoch 8/20\n",
      "75/75 [==============================] - 4s 60ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0176 - val_mean_squared_error: 0.0176\n",
      "Epoch 9/20\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0178 - val_mean_squared_error: 0.0178\n",
      "Epoch 10/20\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 0.0079 - mean_squared_error: 0.0079 - val_loss: 0.0181 - val_mean_squared_error: 0.0181\n",
      "Epoch 11/20\n",
      "75/75 [==============================] - 4s 58ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0175 - val_mean_squared_error: 0.0175\n",
      "Epoch 12/20\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
      "Epoch 13/20\n",
      "75/75 [==============================] - 4s 58ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0172 - val_mean_squared_error: 0.0172\n",
      "Epoch 14/20\n",
      "75/75 [==============================] - 4s 60ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0171 - val_mean_squared_error: 0.0171\n",
      "Epoch 15/20\n",
      "75/75 [==============================] - 4s 58ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0170 - val_mean_squared_error: 0.0170\n",
      "Epoch 16/20\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0172 - val_mean_squared_error: 0.0172\n",
      "Epoch 17/20\n",
      "75/75 [==============================] - 4s 60ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0171 - val_mean_squared_error: 0.0171\n",
      "Epoch 18/20\n",
      "75/75 [==============================] - 5s 62ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0167 - val_mean_squared_error: 0.0167\n",
      "Epoch 19/20\n",
      "75/75 [==============================] - 5s 66ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0167 - val_mean_squared_error: 0.0167\n",
      "Epoch 20/20\n",
      "75/75 [==============================] - 5s 65ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0168 - val_mean_squared_error: 0.0168\n"
     ]
    }
   ],
   "source": [
    "# Split the 3D head pose labels for training, validation, and test sets\n",
    "pose_labels = labels[:, 1:]\n",
    "pose_labels_train, pose_labels_test = train_test_split(pose_labels, test_size=0.2, random_state=42)\n",
    "pose_labels_train, pose_labels_val = train_test_split(pose_labels_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "regression_training_history = regression_cnn_model.fit(\n",
    "    images_train, pose_labels_train, \n",
    "    epochs=20, \n",
    "    validation_data=(images_val, pose_labels_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38d545da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Test MSE: 0.017634311690926552\n",
      "25/25 [==============================] - 0s 12ms/step\n",
      "R-squared for 3D head pose prediction: 0.33458403762706473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_mse = regression_cnn_model.evaluate(images_test, pose_labels_test)\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = regression_cnn_model.predict(images_test)\n",
    "\n",
    "# Calculate R-squared (R²) for regression\n",
    "r_squared = r2_score(pose_labels_test, predictions)\n",
    "print(f\"R-squared for 3D head pose prediction: {r_squared}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
